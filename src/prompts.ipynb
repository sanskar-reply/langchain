{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Model Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tell me a gruesome joke about goats.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"Tell me a {adjective} joke about {content}.\"\n",
    ")\n",
    "prompt_template.format(adjective=\"gruesome\", content=\"goats\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "8 validation errors for ChatPromptTemplate\nmessages -> 0\n  value is not a valid dict (type=type_error.dict)\nmessages -> 0\n  value is not a valid dict (type=type_error.dict)\nmessages -> 1\n  value is not a valid dict (type=type_error.dict)\nmessages -> 1\n  value is not a valid dict (type=type_error.dict)\nmessages -> 2\n  value is not a valid dict (type=type_error.dict)\nmessages -> 2\n  value is not a valid dict (type=type_error.dict)\nmessages -> 3\n  value is not a valid dict (type=type_error.dict)\nmessages -> 3\n  value is not a valid dict (type=type_error.dict)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprompts\u001b[39;00m \u001b[39mimport\u001b[39;00m ChatPromptTemplate\n\u001b[0;32m----> 3\u001b[0m template \u001b[39m=\u001b[39m ChatPromptTemplate\u001b[39m.\u001b[39mfrom_messages([\n\u001b[1;32m      4\u001b[0m     (\u001b[39m\"\u001b[39m\u001b[39msystem\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mYou are a helpful AI bot. Your name is \u001b[39m\u001b[39m{name}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m      5\u001b[0m     (\u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mHello, how are you doing?\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m      6\u001b[0m     (\u001b[39m\"\u001b[39m\u001b[39mai\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mI\u001b[39m\u001b[39m'\u001b[39m\u001b[39mm doing well, thanks!\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m      7\u001b[0m     (\u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m{user_input}\u001b[39;00m\u001b[39m\"\u001b[39m),\n\u001b[1;32m      8\u001b[0m ])\n\u001b[1;32m     10\u001b[0m messages \u001b[39m=\u001b[39m template\u001b[39m.\u001b[39mformat_messages(\n\u001b[1;32m     11\u001b[0m     name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBob\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m     user_input\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mWhat is your name?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     13\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langchain/prompts/chat.py:195\u001b[0m, in \u001b[0;36mChatPromptTemplate.from_messages\u001b[0;34m(cls, messages)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(message, BaseMessagePromptTemplate):\n\u001b[1;32m    194\u001b[0m         input_vars\u001b[39m.\u001b[39mupdate(message\u001b[39m.\u001b[39minput_variables)\n\u001b[0;32m--> 195\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m(input_variables\u001b[39m=\u001b[39m\u001b[39mlist\u001b[39m(input_vars), messages\u001b[39m=\u001b[39mmessages)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/pydantic/main.py:341\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValidationError\u001b[0m: 8 validation errors for ChatPromptTemplate\nmessages -> 0\n  value is not a valid dict (type=type_error.dict)\nmessages -> 0\n  value is not a valid dict (type=type_error.dict)\nmessages -> 1\n  value is not a valid dict (type=type_error.dict)\nmessages -> 1\n  value is not a valid dict (type=type_error.dict)\nmessages -> 2\n  value is not a valid dict (type=type_error.dict)\nmessages -> 2\n  value is not a valid dict (type=type_error.dict)\nmessages -> 3\n  value is not a valid dict (type=type_error.dict)\nmessages -> 3\n  value is not a valid dict (type=type_error.dict)"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n",
    "    (\"human\", \"Hello, how are you doing?\"),\n",
    "    (\"ai\", \"I'm doing well, thanks!\"),\n",
    "    (\"human\", \"{user_input}\"),\n",
    "])\n",
    "\n",
    "messages = template.format_messages(\n",
    "    name=\"Bob\",\n",
    "    user_input=\"What is your name?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I absolutely adore indulging in delicious treats!', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.prompts.chat import SystemMessage, HumanMessagePromptTemplate\n",
    "\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=(\n",
    "                \"You are a helpful assistant that re-writes the user's text to \"\n",
    "                \"sound more upbeat.\"\n",
    "            )\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(\"{text}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "llm(template.format_messages(text='i dont like eating tasty things.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tell me a offensive joke about racism'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"Tell me a {adjective} joke about {topic}\"\n",
    ")\n",
    "\n",
    "prompt_template.format(adjective=\"offensive\", topic=\"racism\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "from langchain.chat_models import ChatVertexAI\n",
    "from langchain.embeddings import VertexAIEmbeddings\n",
    "from langchain.llms import VertexAI\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "import vertexai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"gen-ai-sandbox\"  # @param {type:\"string\"}\n",
    "vertexai.init(project=PROJECT_ID, location=\"us-central1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions for Embeddings API with rate limiting\n",
    "def rate_limit(max_per_minute):\n",
    "    period = 60 / max_per_minute\n",
    "    print(\"Waiting\")\n",
    "    while True:\n",
    "        before = time.time()\n",
    "        yield\n",
    "        after = time.time()\n",
    "        elapsed = after - before\n",
    "        sleep_time = max(0, period - elapsed)\n",
    "        if sleep_time > 0:\n",
    "            print(\".\", end=\"\")\n",
    "            time.sleep(sleep_time)\n",
    "\n",
    "\n",
    "class CustomVertexAIEmbeddings(VertexAIEmbeddings, BaseModel):\n",
    "    requests_per_minute: int\n",
    "    num_instances_per_batch: int\n",
    "\n",
    "    # Overriding embed_documents method\n",
    "    def embed_documents(self, texts: List[str]):\n",
    "        limiter = rate_limit(self.requests_per_minute)\n",
    "        results = []\n",
    "        docs = list(texts)\n",
    "\n",
    "        while docs:\n",
    "            # Working in batches because the API accepts maximum 5\n",
    "            # documents per request to get embeddings\n",
    "            head, docs = (\n",
    "                docs[: self.num_instances_per_batch],\n",
    "                docs[self.num_instances_per_batch :],\n",
    "            )\n",
    "            chunk = self.client.get_embeddings(head)\n",
    "            results.extend(chunk)\n",
    "            next(limiter)\n",
    "\n",
    "        return [r.values for r in results]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining all the models here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM model\n",
    "llm = VertexAI(\n",
    "    model_name=\"text-bison@001\",\n",
    "    max_output_tokens=256,\n",
    "    temperature=0.1,\n",
    "    top_p=0.8,\n",
    "    top_k=40,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Chat\n",
    "chat = ChatVertexAI()\n",
    "\n",
    "# Embedding\n",
    "# EMBEDDING_QPM = 100\n",
    "# EMBEDDING_NUM_BATCH = 5\n",
    "# embeddings = CustomVertexAIEmbeddings(\n",
    "#     requests_per_minute=EMBEDDING_QPM,\n",
    "#     num_instances_per_batch=EMBEDDING_NUM_BATCH,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'google.protobuf.pyext._message.RepeatedCompositeCo' object has no attribute 'WhichOneof'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m my_text \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mWhat day comes after Friday?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[39mprint\u001b[39m(llm(my_text))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langchain/llms/base.py:297\u001b[0m, in \u001b[0;36mBaseLLM.__call__\u001b[0;34m(self, prompt, stop, callbacks)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(prompt, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    291\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    292\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mArgument `prompt` is expected to be a string. Instead found \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    293\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(prompt)\u001b[39m}\u001b[39;00m\u001b[39m. If you want to run the LLM on multiple prompts, use \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    294\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`generate` instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    295\u001b[0m     )\n\u001b[1;32m    296\u001b[0m \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 297\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate([prompt], stop\u001b[39m=\u001b[39mstop, callbacks\u001b[39m=\u001b[39mcallbacks)\n\u001b[1;32m    298\u001b[0m     \u001b[39m.\u001b[39mgenerations[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[1;32m    299\u001b[0m     \u001b[39m.\u001b[39mtext\n\u001b[1;32m    300\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langchain/llms/base.py:191\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    190\u001b[0m     run_manager\u001b[39m.\u001b[39mon_llm_error(e)\n\u001b[0;32m--> 191\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    192\u001b[0m run_manager\u001b[39m.\u001b[39mon_llm_end(output)\n\u001b[1;32m    193\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langchain/llms/base.py:185\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks)\u001b[0m\n\u001b[1;32m    180\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_llm_start(\n\u001b[1;32m    181\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m}, prompts, invocation_params\u001b[39m=\u001b[39mparams\n\u001b[1;32m    182\u001b[0m )\n\u001b[1;32m    183\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    184\u001b[0m     output \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 185\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(prompts, stop\u001b[39m=\u001b[39mstop, run_manager\u001b[39m=\u001b[39mrun_manager)\n\u001b[1;32m    186\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    187\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(prompts, stop\u001b[39m=\u001b[39mstop)\n\u001b[1;32m    188\u001b[0m     )\n\u001b[1;32m    189\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    190\u001b[0m     run_manager\u001b[39m.\u001b[39mon_llm_error(e)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langchain/llms/base.py:436\u001b[0m, in \u001b[0;36mLLM._generate\u001b[0;34m(self, prompts, stop, run_manager)\u001b[0m\n\u001b[1;32m    433\u001b[0m new_arg_supported \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39msignature(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call)\u001b[39m.\u001b[39mparameters\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mrun_manager\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    434\u001b[0m \u001b[39mfor\u001b[39;00m prompt \u001b[39min\u001b[39;00m prompts:\n\u001b[1;32m    435\u001b[0m     text \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 436\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(prompt, stop\u001b[39m=\u001b[39mstop, run_manager\u001b[39m=\u001b[39mrun_manager)\n\u001b[1;32m    437\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    438\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(prompt, stop\u001b[39m=\u001b[39mstop)\n\u001b[1;32m    439\u001b[0m     )\n\u001b[1;32m    440\u001b[0m     generations\u001b[39m.\u001b[39mappend([Generation(text\u001b[39m=\u001b[39mtext)])\n\u001b[1;32m    441\u001b[0m \u001b[39mreturn\u001b[39;00m LLMResult(generations\u001b[39m=\u001b[39mgenerations)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langchain/llms/vertexai.py:110\u001b[0m, in \u001b[0;36mVertexAI._call\u001b[0;34m(self, prompt, stop, run_manager)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call\u001b[39m(\n\u001b[1;32m     95\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     96\u001b[0m     prompt: \u001b[39mstr\u001b[39m,\n\u001b[1;32m     97\u001b[0m     stop: Optional[List[\u001b[39mstr\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     98\u001b[0m     run_manager: Optional[CallbackManagerForLLMRun] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     99\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m    100\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Call Vertex model to get predictions based on the prompt.\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \n\u001b[1;32m    102\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[39m        The string generated by the model.\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_predict(prompt, stop)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langchain/llms/vertexai.py:52\u001b[0m, in \u001b[0;36m_VertexAICommon._predict\u001b[0;34m(self, prompt, stop)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_predict\u001b[39m(\u001b[39mself\u001b[39m, prompt: \u001b[39mstr\u001b[39m, stop: Optional[List[\u001b[39mstr\u001b[39m]]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[0;32m---> 52\u001b[0m     res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclient\u001b[39m.\u001b[39mpredict(prompt, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_default_params)\n\u001b[1;32m     53\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_enforce_stop_words(res\u001b[39m.\u001b[39mtext, stop)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/vertexai/language_models/_language_models.py:655\u001b[0m, in \u001b[0;36m_TextGenerationModel.predict\u001b[0;34m(self, prompt, max_output_tokens, temperature, top_k, top_p, stop_sequences)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    633\u001b[0m     prompt: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    639\u001b[0m     stop_sequences: Optional[List[\u001b[39mstr\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    640\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mTextGenerationResponse\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    641\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Gets model response for a single prompt.\u001b[39;00m\n\u001b[1;32m    642\u001b[0m \n\u001b[1;32m    643\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    652\u001b[0m \u001b[39m        A `TextGenerationResponse` object that contains the text produced by the model.\u001b[39;00m\n\u001b[1;32m    653\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 655\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_predict(\n\u001b[1;32m    656\u001b[0m         prompts\u001b[39m=\u001b[39m[prompt],\n\u001b[1;32m    657\u001b[0m         max_output_tokens\u001b[39m=\u001b[39mmax_output_tokens,\n\u001b[1;32m    658\u001b[0m         temperature\u001b[39m=\u001b[39mtemperature,\n\u001b[1;32m    659\u001b[0m         top_k\u001b[39m=\u001b[39mtop_k,\n\u001b[1;32m    660\u001b[0m         top_p\u001b[39m=\u001b[39mtop_p,\n\u001b[1;32m    661\u001b[0m         stop_sequences\u001b[39m=\u001b[39mstop_sequences,\n\u001b[1;32m    662\u001b[0m     )[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/vertexai/language_models/_language_models.py:704\u001b[0m, in \u001b[0;36m_TextGenerationModel._batch_predict\u001b[0;34m(self, prompts, max_output_tokens, temperature, top_k, top_p, stop_sequences)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[39mif\u001b[39;00m stop_sequences:\n\u001b[1;32m    702\u001b[0m     prediction_parameters[\u001b[39m\"\u001b[39m\u001b[39mstopSequences\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m stop_sequences\n\u001b[0;32m--> 704\u001b[0m prediction_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_endpoint\u001b[39m.\u001b[39mpredict(\n\u001b[1;32m    705\u001b[0m     instances\u001b[39m=\u001b[39minstances,\n\u001b[1;32m    706\u001b[0m     parameters\u001b[39m=\u001b[39mprediction_parameters,\n\u001b[1;32m    707\u001b[0m )\n\u001b[1;32m    709\u001b[0m results \u001b[39m=\u001b[39m []\n\u001b[1;32m    710\u001b[0m \u001b[39mfor\u001b[39;00m prediction \u001b[39min\u001b[39;00m prediction_response\u001b[39m.\u001b[39mpredictions:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/google/cloud/aiplatform/models.py:1564\u001b[0m, in \u001b[0;36mEndpoint.predict\u001b[0;34m(self, instances, parameters, timeout, use_raw_predict)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[39mreturn\u001b[39;00m Prediction(\n\u001b[1;32m   1552\u001b[0m         predictions\u001b[39m=\u001b[39mjson_response[\u001b[39m\"\u001b[39m\u001b[39mpredictions\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m   1553\u001b[0m         deployed_model_id\u001b[39m=\u001b[39mraw_predict_response\u001b[39m.\u001b[39mheaders[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1561\u001b[0m         ),\n\u001b[1;32m   1562\u001b[0m     )\n\u001b[1;32m   1563\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1564\u001b[0m     prediction_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prediction_client\u001b[39m.\u001b[39mpredict(\n\u001b[1;32m   1565\u001b[0m         endpoint\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gca_resource\u001b[39m.\u001b[39mname,\n\u001b[1;32m   1566\u001b[0m         instances\u001b[39m=\u001b[39minstances,\n\u001b[1;32m   1567\u001b[0m         parameters\u001b[39m=\u001b[39mparameters,\n\u001b[1;32m   1568\u001b[0m         timeout\u001b[39m=\u001b[39mtimeout,\n\u001b[1;32m   1569\u001b[0m     )\n\u001b[1;32m   1571\u001b[0m     \u001b[39mreturn\u001b[39;00m Prediction(\n\u001b[1;32m   1572\u001b[0m         predictions\u001b[39m=\u001b[39m[\n\u001b[1;32m   1573\u001b[0m             json_format\u001b[39m.\u001b[39mMessageToDict(item)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1578\u001b[0m         model_resource_name\u001b[39m=\u001b[39mprediction_response\u001b[39m.\u001b[39mmodel,\n\u001b[1;32m   1579\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/google/cloud/aiplatform_v1/services/prediction_service/client.py:589\u001b[0m, in \u001b[0;36mPredictionServiceClient.predict\u001b[0;34m(self, request, endpoint, instances, parameters, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    587\u001b[0m     request\u001b[39m.\u001b[39mendpoint \u001b[39m=\u001b[39m endpoint\n\u001b[1;32m    588\u001b[0m \u001b[39mif\u001b[39;00m instances \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 589\u001b[0m     request\u001b[39m.\u001b[39minstances\u001b[39m.\u001b[39mextend(instances)\n\u001b[1;32m    590\u001b[0m \u001b[39mif\u001b[39;00m parameters \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    591\u001b[0m     request\u001b[39m.\u001b[39mparameters \u001b[39m=\u001b[39m parameters\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/proto/message.py:752\u001b[0m, in \u001b[0;36mMessage.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    750\u001b[0m pb_value \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pb, key)\n\u001b[1;32m    751\u001b[0m marshal \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_meta\u001b[39m.\u001b[39mmarshal\n\u001b[0;32m--> 752\u001b[0m \u001b[39mreturn\u001b[39;00m marshal\u001b[39m.\u001b[39mto_python(pb_type, pb_value, absent\u001b[39m=\u001b[39mkey \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/proto/marshal/marshal.py:193\u001b[0m, in \u001b[0;36mBaseMarshal.to_python\u001b[0;34m(self, proto_type, value, absent)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[39mif\u001b[39;00m value_type \u001b[39min\u001b[39;00m compat\u001b[39m.\u001b[39mmap_composite_types:\n\u001b[1;32m    192\u001b[0m     \u001b[39mreturn\u001b[39;00m MapComposite(value, marshal\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n\u001b[0;32m--> 193\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_rule(proto_type\u001b[39m=\u001b[39mproto_type)\u001b[39m.\u001b[39mto_python(value, absent\u001b[39m=\u001b[39mabsent)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/google/cloud/aiplatform/utils/enhanced_library/_decorators.py:24\u001b[0m, in \u001b[0;36mConversionValueRule.to_python\u001b[0;34m(self, value, absent)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mto_python\u001b[39m(\u001b[39mself\u001b[39m, value, \u001b[39m*\u001b[39m, absent: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m---> 24\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mto_python(value, absent\u001b[39m=\u001b[39mabsent)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/proto/marshal/rules/struct.py:39\u001b[0m, in \u001b[0;36mValueRule.to_python\u001b[0;34m(self, value, absent)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mto_python\u001b[39m(\u001b[39mself\u001b[39m, value, \u001b[39m*\u001b[39m, absent: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     30\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Coerce the given value to the appropriate Python type.\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \n\u001b[1;32m     32\u001b[0m \u001b[39m    Note that both NullValue and absent fields return None.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[39m    which is True for NullValue and False for an absent value.\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m     kind \u001b[39m=\u001b[39m value\u001b[39m.\u001b[39mWhichOneof(\u001b[39m\"\u001b[39m\u001b[39mkind\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     40\u001b[0m     \u001b[39mif\u001b[39;00m kind \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnull_value\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m absent:\n\u001b[1;32m     41\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'google.protobuf.pyext._message.RepeatedCompositeCo' object has no attribute 'WhichOneof'"
     ]
    }
   ],
   "source": [
    "my_text = \"What day comes after Friday?\"\n",
    "\n",
    "print(llm(my_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# palm api stuff\n",
    "from langchain.llms.vertexai import VertexAI\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# llm = VertexAI(model_name=\"code-bison\")\n",
    "# llm(\"say hello\")\n",
    "# llm = VertexAI(model_name=\"text-bison\")\n",
    "llm = OpenAI(temperature=0.9)\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"What is a good name for a company that makes {product}?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Bright Toe Socks\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Run the chain only specifying the input variable.\n",
    "print(chain.run(\"colorful socks\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
