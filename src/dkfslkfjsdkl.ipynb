{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import aiplatform\n",
    "from langchain.chat_models import ChatVertexAI\n",
    "from langchain.embeddings import VertexAIEmbeddings\n",
    "from langchain.llms import VertexAI\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from typing import List\n",
    "import vertexai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'google.protobuf.pyext._message.RepeatedCompositeCo' object has no attribute 'WhichOneof'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m prompt_template \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mTell me a \u001b[39m\u001b[39m{adjective}\u001b[39;00m\u001b[39m joke\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m llm_chain \u001b[39m=\u001b[39m LLMChain(llm\u001b[39m=\u001b[39mchat, prompt\u001b[39m=\u001b[39mPromptTemplate\u001b[39m.\u001b[39mfrom_template(prompt_template))\n\u001b[0;32m----> 5\u001b[0m llm_chain(inputs\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39madjective\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mcorny\u001b[39m\u001b[39m\"\u001b[39m})\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langchain/chains/base.py:140\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    139\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 140\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    141\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    142\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(inputs, outputs, return_only_outputs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langchain/chains/base.py:134\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks)\u001b[0m\n\u001b[1;32m    128\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    129\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m},\n\u001b[1;32m    130\u001b[0m     inputs,\n\u001b[1;32m    131\u001b[0m )\n\u001b[1;32m    132\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 134\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs, run_manager\u001b[39m=\u001b[39mrun_manager)\n\u001b[1;32m    135\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    136\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[1;32m    137\u001b[0m     )\n\u001b[1;32m    138\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    139\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langchain/chains/llm.py:69\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call\u001b[39m(\n\u001b[1;32m     65\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     66\u001b[0m     inputs: Dict[\u001b[39mstr\u001b[39m, Any],\n\u001b[1;32m     67\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     68\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m---> 69\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate([inputs], run_manager\u001b[39m=\u001b[39mrun_manager)\n\u001b[1;32m     70\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_outputs(response)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langchain/chains/llm.py:79\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Generate LLM result from inputs.\"\"\"\u001b[39;00m\n\u001b[1;32m     78\u001b[0m prompts, stop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_prompts(input_list, run_manager\u001b[39m=\u001b[39mrun_manager)\n\u001b[0;32m---> 79\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm\u001b[39m.\u001b[39mgenerate_prompt(\n\u001b[1;32m     80\u001b[0m     prompts, stop, callbacks\u001b[39m=\u001b[39mrun_manager\u001b[39m.\u001b[39mget_child() \u001b[39mif\u001b[39;00m run_manager \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     81\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langchain/chat_models/base.py:143\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_prompt\u001b[39m(\n\u001b[1;32m    137\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    138\u001b[0m     prompts: List[PromptValue],\n\u001b[1;32m    139\u001b[0m     stop: Optional[List[\u001b[39mstr\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    140\u001b[0m     callbacks: Callbacks \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    141\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[1;32m    142\u001b[0m     prompt_messages \u001b[39m=\u001b[39m [p\u001b[39m.\u001b[39mto_messages() \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m prompts]\n\u001b[0;32m--> 143\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate(prompt_messages, stop\u001b[39m=\u001b[39mstop, callbacks\u001b[39m=\u001b[39mcallbacks)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langchain/chat_models/base.py:91\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     90\u001b[0m     run_manager\u001b[39m.\u001b[39mon_llm_error(e)\n\u001b[0;32m---> 91\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m     92\u001b[0m llm_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_combine_llm_outputs([res\u001b[39m.\u001b[39mllm_output \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results])\n\u001b[1;32m     93\u001b[0m generations \u001b[39m=\u001b[39m [res\u001b[39m.\u001b[39mgenerations \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langchain/chat_models/base.py:83\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks)\u001b[0m\n\u001b[1;32m     79\u001b[0m new_arg_supported \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39msignature(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate)\u001b[39m.\u001b[39mparameters\u001b[39m.\u001b[39mget(\n\u001b[1;32m     80\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mrun_manager\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     81\u001b[0m )\n\u001b[1;32m     82\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 83\u001b[0m     results \u001b[39m=\u001b[39m [\n\u001b[1;32m     84\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(m, stop\u001b[39m=\u001b[39mstop, run_manager\u001b[39m=\u001b[39mrun_manager)\n\u001b[1;32m     85\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m     86\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(m, stop\u001b[39m=\u001b[39mstop)\n\u001b[1;32m     87\u001b[0m         \u001b[39mfor\u001b[39;00m m \u001b[39min\u001b[39;00m messages\n\u001b[1;32m     88\u001b[0m     ]\n\u001b[1;32m     89\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     90\u001b[0m     run_manager\u001b[39m.\u001b[39mon_llm_error(e)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langchain/chat_models/base.py:84\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     79\u001b[0m new_arg_supported \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39msignature(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate)\u001b[39m.\u001b[39mparameters\u001b[39m.\u001b[39mget(\n\u001b[1;32m     80\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mrun_manager\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     81\u001b[0m )\n\u001b[1;32m     82\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     83\u001b[0m     results \u001b[39m=\u001b[39m [\n\u001b[0;32m---> 84\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(m, stop\u001b[39m=\u001b[39mstop, run_manager\u001b[39m=\u001b[39mrun_manager)\n\u001b[1;32m     85\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m     86\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(m, stop\u001b[39m=\u001b[39mstop)\n\u001b[1;32m     87\u001b[0m         \u001b[39mfor\u001b[39;00m m \u001b[39min\u001b[39;00m messages\n\u001b[1;32m     88\u001b[0m     ]\n\u001b[1;32m     89\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     90\u001b[0m     run_manager\u001b[39m.\u001b[39mon_llm_error(e)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langchain/chat_models/vertexai.py:125\u001b[0m, in \u001b[0;36mChatVertexAI._generate\u001b[0;34m(self, messages, stop, run_manager)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[39mfor\u001b[39;00m pair \u001b[39min\u001b[39;00m history\u001b[39m.\u001b[39mhistory:\n\u001b[1;32m    124\u001b[0m     chat\u001b[39m.\u001b[39m_history\u001b[39m.\u001b[39mappend((pair\u001b[39m.\u001b[39mquestion\u001b[39m.\u001b[39mcontent, pair\u001b[39m.\u001b[39manswer\u001b[39m.\u001b[39mcontent))\n\u001b[0;32m--> 125\u001b[0m response \u001b[39m=\u001b[39m chat\u001b[39m.\u001b[39msend_message(question\u001b[39m.\u001b[39mcontent, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_default_params)\n\u001b[1;32m    126\u001b[0m text \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_enforce_stop_words(response\u001b[39m.\u001b[39mtext, stop)\n\u001b[1;32m    127\u001b[0m \u001b[39mreturn\u001b[39;00m ChatResult(generations\u001b[39m=\u001b[39m[ChatGeneration(message\u001b[39m=\u001b[39mAIMessage(content\u001b[39m=\u001b[39mtext))])\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/vertexai/language_models/_language_models.py:1478\u001b[0m, in \u001b[0;36m_ChatSessionBase.send_message\u001b[0;34m(self, message, max_output_tokens, temperature, top_k, top_p, stop_sequences)\u001b[0m\n\u001b[1;32m   1452\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Sends message to the language model and gets a response.\u001b[39;00m\n\u001b[1;32m   1453\u001b[0m \n\u001b[1;32m   1454\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1467\u001b[0m \u001b[39m    A `TextGenerationResponse` object that contains the text produced by the model.\u001b[39;00m\n\u001b[1;32m   1468\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1469\u001b[0m prediction_request \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_request(\n\u001b[1;32m   1470\u001b[0m     message\u001b[39m=\u001b[39mmessage,\n\u001b[1;32m   1471\u001b[0m     max_output_tokens\u001b[39m=\u001b[39mmax_output_tokens,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1475\u001b[0m     stop_sequences\u001b[39m=\u001b[39mstop_sequences,\n\u001b[1;32m   1476\u001b[0m )\n\u001b[0;32m-> 1478\u001b[0m prediction_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_model\u001b[39m.\u001b[39m_endpoint\u001b[39m.\u001b[39mpredict(\n\u001b[1;32m   1479\u001b[0m     instances\u001b[39m=\u001b[39m[prediction_request\u001b[39m.\u001b[39minstance],\n\u001b[1;32m   1480\u001b[0m     parameters\u001b[39m=\u001b[39mprediction_request\u001b[39m.\u001b[39mparameters,\n\u001b[1;32m   1481\u001b[0m )\n\u001b[1;32m   1482\u001b[0m response_obj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parse_chat_prediction_response(\n\u001b[1;32m   1483\u001b[0m     prediction_response\u001b[39m=\u001b[39mprediction_response\n\u001b[1;32m   1484\u001b[0m )\n\u001b[1;32m   1485\u001b[0m response_text \u001b[39m=\u001b[39m response_obj\u001b[39m.\u001b[39mtext\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/google/cloud/aiplatform/models.py:1564\u001b[0m, in \u001b[0;36mEndpoint.predict\u001b[0;34m(self, instances, parameters, timeout, use_raw_predict)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[39mreturn\u001b[39;00m Prediction(\n\u001b[1;32m   1552\u001b[0m         predictions\u001b[39m=\u001b[39mjson_response[\u001b[39m\"\u001b[39m\u001b[39mpredictions\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m   1553\u001b[0m         deployed_model_id\u001b[39m=\u001b[39mraw_predict_response\u001b[39m.\u001b[39mheaders[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1561\u001b[0m         ),\n\u001b[1;32m   1562\u001b[0m     )\n\u001b[1;32m   1563\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1564\u001b[0m     prediction_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prediction_client\u001b[39m.\u001b[39mpredict(\n\u001b[1;32m   1565\u001b[0m         endpoint\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gca_resource\u001b[39m.\u001b[39mname,\n\u001b[1;32m   1566\u001b[0m         instances\u001b[39m=\u001b[39minstances,\n\u001b[1;32m   1567\u001b[0m         parameters\u001b[39m=\u001b[39mparameters,\n\u001b[1;32m   1568\u001b[0m         timeout\u001b[39m=\u001b[39mtimeout,\n\u001b[1;32m   1569\u001b[0m     )\n\u001b[1;32m   1571\u001b[0m     \u001b[39mreturn\u001b[39;00m Prediction(\n\u001b[1;32m   1572\u001b[0m         predictions\u001b[39m=\u001b[39m[\n\u001b[1;32m   1573\u001b[0m             json_format\u001b[39m.\u001b[39mMessageToDict(item)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1578\u001b[0m         model_resource_name\u001b[39m=\u001b[39mprediction_response\u001b[39m.\u001b[39mmodel,\n\u001b[1;32m   1579\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/google/cloud/aiplatform_v1/services/prediction_service/client.py:589\u001b[0m, in \u001b[0;36mPredictionServiceClient.predict\u001b[0;34m(self, request, endpoint, instances, parameters, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    587\u001b[0m     request\u001b[39m.\u001b[39mendpoint \u001b[39m=\u001b[39m endpoint\n\u001b[1;32m    588\u001b[0m \u001b[39mif\u001b[39;00m instances \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 589\u001b[0m     request\u001b[39m.\u001b[39minstances\u001b[39m.\u001b[39mextend(instances)\n\u001b[1;32m    590\u001b[0m \u001b[39mif\u001b[39;00m parameters \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    591\u001b[0m     request\u001b[39m.\u001b[39mparameters \u001b[39m=\u001b[39m parameters\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/proto/message.py:752\u001b[0m, in \u001b[0;36mMessage.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    750\u001b[0m pb_value \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pb, key)\n\u001b[1;32m    751\u001b[0m marshal \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_meta\u001b[39m.\u001b[39mmarshal\n\u001b[0;32m--> 752\u001b[0m \u001b[39mreturn\u001b[39;00m marshal\u001b[39m.\u001b[39mto_python(pb_type, pb_value, absent\u001b[39m=\u001b[39mkey \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/proto/marshal/marshal.py:193\u001b[0m, in \u001b[0;36mBaseMarshal.to_python\u001b[0;34m(self, proto_type, value, absent)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[39mif\u001b[39;00m value_type \u001b[39min\u001b[39;00m compat\u001b[39m.\u001b[39mmap_composite_types:\n\u001b[1;32m    192\u001b[0m     \u001b[39mreturn\u001b[39;00m MapComposite(value, marshal\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n\u001b[0;32m--> 193\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_rule(proto_type\u001b[39m=\u001b[39mproto_type)\u001b[39m.\u001b[39mto_python(value, absent\u001b[39m=\u001b[39mabsent)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/google/cloud/aiplatform/utils/enhanced_library/_decorators.py:24\u001b[0m, in \u001b[0;36mConversionValueRule.to_python\u001b[0;34m(self, value, absent)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mto_python\u001b[39m(\u001b[39mself\u001b[39m, value, \u001b[39m*\u001b[39m, absent: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m---> 24\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mto_python(value, absent\u001b[39m=\u001b[39mabsent)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/proto/marshal/rules/struct.py:39\u001b[0m, in \u001b[0;36mValueRule.to_python\u001b[0;34m(self, value, absent)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mto_python\u001b[39m(\u001b[39mself\u001b[39m, value, \u001b[39m*\u001b[39m, absent: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     30\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Coerce the given value to the appropriate Python type.\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \n\u001b[1;32m     32\u001b[0m \u001b[39m    Note that both NullValue and absent fields return None.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[39m    which is True for NullValue and False for an absent value.\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m     kind \u001b[39m=\u001b[39m value\u001b[39m.\u001b[39mWhichOneof(\u001b[39m\"\u001b[39m\u001b[39mkind\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     40\u001b[0m     \u001b[39mif\u001b[39;00m kind \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnull_value\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m absent:\n\u001b[1;32m     41\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'google.protobuf.pyext._message.RepeatedCompositeCo' object has no attribute 'WhichOneof'"
     ]
    }
   ],
   "source": [
    "chat = ChatVertexAI()\n",
    "prompt_template = \"Tell me a {adjective} joke\"\n",
    "llm_chain = LLMChain(llm=chat, prompt=PromptTemplate.from_template(prompt_template))\n",
    "\n",
    "llm_chain(inputs={\"adjective\": \"corny\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import VertexAI\n",
    "from langchain import PromptTemplate, LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = VertexAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'google.protobuf.pyext._message.RepeatedCompositeCo' object has no attribute 'WhichOneof'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m question \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mWhat NFL team won the Super Bowl in the year Justin Beiber was born?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m llm_chain\u001b[39m.\u001b[39mrun(question)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langchain/chains/base.py:236\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, *args, **kwargs)\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    235\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`run` supports only one positional argument.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 236\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(args[\u001b[39m0\u001b[39m], callbacks\u001b[39m=\u001b[39mcallbacks)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_keys[\u001b[39m0\u001b[39m]]\n\u001b[1;32m    238\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[1;32m    239\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(kwargs, callbacks\u001b[39m=\u001b[39mcallbacks)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_keys[\u001b[39m0\u001b[39m]]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langchain/chains/base.py:140\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    139\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 140\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    141\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    142\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(inputs, outputs, return_only_outputs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langchain/chains/base.py:134\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks)\u001b[0m\n\u001b[1;32m    128\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    129\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m},\n\u001b[1;32m    130\u001b[0m     inputs,\n\u001b[1;32m    131\u001b[0m )\n\u001b[1;32m    132\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 134\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs, run_manager\u001b[39m=\u001b[39mrun_manager)\n\u001b[1;32m    135\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    136\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[1;32m    137\u001b[0m     )\n\u001b[1;32m    138\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    139\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langchain/chains/llm.py:69\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call\u001b[39m(\n\u001b[1;32m     65\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     66\u001b[0m     inputs: Dict[\u001b[39mstr\u001b[39m, Any],\n\u001b[1;32m     67\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     68\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m---> 69\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate([inputs], run_manager\u001b[39m=\u001b[39mrun_manager)\n\u001b[1;32m     70\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_outputs(response)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langchain/chains/llm.py:79\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Generate LLM result from inputs.\"\"\"\u001b[39;00m\n\u001b[1;32m     78\u001b[0m prompts, stop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_prompts(input_list, run_manager\u001b[39m=\u001b[39mrun_manager)\n\u001b[0;32m---> 79\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm\u001b[39m.\u001b[39mgenerate_prompt(\n\u001b[1;32m     80\u001b[0m     prompts, stop, callbacks\u001b[39m=\u001b[39mrun_manager\u001b[39m.\u001b[39mget_child() \u001b[39mif\u001b[39;00m run_manager \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     81\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langchain/llms/base.py:134\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_prompt\u001b[39m(\n\u001b[1;32m    128\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    129\u001b[0m     prompts: List[PromptValue],\n\u001b[1;32m    130\u001b[0m     stop: Optional[List[\u001b[39mstr\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    131\u001b[0m     callbacks: Callbacks \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    132\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[1;32m    133\u001b[0m     prompt_strings \u001b[39m=\u001b[39m [p\u001b[39m.\u001b[39mto_string() \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m prompts]\n\u001b[0;32m--> 134\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate(prompt_strings, stop\u001b[39m=\u001b[39mstop, callbacks\u001b[39m=\u001b[39mcallbacks)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langchain/llms/base.py:191\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    190\u001b[0m     run_manager\u001b[39m.\u001b[39mon_llm_error(e)\n\u001b[0;32m--> 191\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    192\u001b[0m run_manager\u001b[39m.\u001b[39mon_llm_end(output)\n\u001b[1;32m    193\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langchain/llms/base.py:185\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks)\u001b[0m\n\u001b[1;32m    180\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_llm_start(\n\u001b[1;32m    181\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m}, prompts, invocation_params\u001b[39m=\u001b[39mparams\n\u001b[1;32m    182\u001b[0m )\n\u001b[1;32m    183\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    184\u001b[0m     output \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 185\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(prompts, stop\u001b[39m=\u001b[39mstop, run_manager\u001b[39m=\u001b[39mrun_manager)\n\u001b[1;32m    186\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    187\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(prompts, stop\u001b[39m=\u001b[39mstop)\n\u001b[1;32m    188\u001b[0m     )\n\u001b[1;32m    189\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    190\u001b[0m     run_manager\u001b[39m.\u001b[39mon_llm_error(e)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langchain/llms/base.py:436\u001b[0m, in \u001b[0;36mLLM._generate\u001b[0;34m(self, prompts, stop, run_manager)\u001b[0m\n\u001b[1;32m    433\u001b[0m new_arg_supported \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39msignature(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call)\u001b[39m.\u001b[39mparameters\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mrun_manager\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    434\u001b[0m \u001b[39mfor\u001b[39;00m prompt \u001b[39min\u001b[39;00m prompts:\n\u001b[1;32m    435\u001b[0m     text \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 436\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(prompt, stop\u001b[39m=\u001b[39mstop, run_manager\u001b[39m=\u001b[39mrun_manager)\n\u001b[1;32m    437\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    438\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(prompt, stop\u001b[39m=\u001b[39mstop)\n\u001b[1;32m    439\u001b[0m     )\n\u001b[1;32m    440\u001b[0m     generations\u001b[39m.\u001b[39mappend([Generation(text\u001b[39m=\u001b[39mtext)])\n\u001b[1;32m    441\u001b[0m \u001b[39mreturn\u001b[39;00m LLMResult(generations\u001b[39m=\u001b[39mgenerations)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langchain/llms/vertexai.py:110\u001b[0m, in \u001b[0;36mVertexAI._call\u001b[0;34m(self, prompt, stop, run_manager)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call\u001b[39m(\n\u001b[1;32m     95\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     96\u001b[0m     prompt: \u001b[39mstr\u001b[39m,\n\u001b[1;32m     97\u001b[0m     stop: Optional[List[\u001b[39mstr\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     98\u001b[0m     run_manager: Optional[CallbackManagerForLLMRun] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     99\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m    100\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Call Vertex model to get predictions based on the prompt.\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \n\u001b[1;32m    102\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[39m        The string generated by the model.\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_predict(prompt, stop)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/langchain/llms/vertexai.py:52\u001b[0m, in \u001b[0;36m_VertexAICommon._predict\u001b[0;34m(self, prompt, stop)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_predict\u001b[39m(\u001b[39mself\u001b[39m, prompt: \u001b[39mstr\u001b[39m, stop: Optional[List[\u001b[39mstr\u001b[39m]]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[0;32m---> 52\u001b[0m     res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclient\u001b[39m.\u001b[39mpredict(prompt, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_default_params)\n\u001b[1;32m     53\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_enforce_stop_words(res\u001b[39m.\u001b[39mtext, stop)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/vertexai/language_models/_language_models.py:655\u001b[0m, in \u001b[0;36m_TextGenerationModel.predict\u001b[0;34m(self, prompt, max_output_tokens, temperature, top_k, top_p, stop_sequences)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    633\u001b[0m     prompt: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    639\u001b[0m     stop_sequences: Optional[List[\u001b[39mstr\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    640\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mTextGenerationResponse\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    641\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Gets model response for a single prompt.\u001b[39;00m\n\u001b[1;32m    642\u001b[0m \n\u001b[1;32m    643\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    652\u001b[0m \u001b[39m        A `TextGenerationResponse` object that contains the text produced by the model.\u001b[39;00m\n\u001b[1;32m    653\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 655\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_predict(\n\u001b[1;32m    656\u001b[0m         prompts\u001b[39m=\u001b[39m[prompt],\n\u001b[1;32m    657\u001b[0m         max_output_tokens\u001b[39m=\u001b[39mmax_output_tokens,\n\u001b[1;32m    658\u001b[0m         temperature\u001b[39m=\u001b[39mtemperature,\n\u001b[1;32m    659\u001b[0m         top_k\u001b[39m=\u001b[39mtop_k,\n\u001b[1;32m    660\u001b[0m         top_p\u001b[39m=\u001b[39mtop_p,\n\u001b[1;32m    661\u001b[0m         stop_sequences\u001b[39m=\u001b[39mstop_sequences,\n\u001b[1;32m    662\u001b[0m     )[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/vertexai/language_models/_language_models.py:704\u001b[0m, in \u001b[0;36m_TextGenerationModel._batch_predict\u001b[0;34m(self, prompts, max_output_tokens, temperature, top_k, top_p, stop_sequences)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[39mif\u001b[39;00m stop_sequences:\n\u001b[1;32m    702\u001b[0m     prediction_parameters[\u001b[39m\"\u001b[39m\u001b[39mstopSequences\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m stop_sequences\n\u001b[0;32m--> 704\u001b[0m prediction_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_endpoint\u001b[39m.\u001b[39mpredict(\n\u001b[1;32m    705\u001b[0m     instances\u001b[39m=\u001b[39minstances,\n\u001b[1;32m    706\u001b[0m     parameters\u001b[39m=\u001b[39mprediction_parameters,\n\u001b[1;32m    707\u001b[0m )\n\u001b[1;32m    709\u001b[0m results \u001b[39m=\u001b[39m []\n\u001b[1;32m    710\u001b[0m \u001b[39mfor\u001b[39;00m prediction \u001b[39min\u001b[39;00m prediction_response\u001b[39m.\u001b[39mpredictions:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/google/cloud/aiplatform/models.py:1564\u001b[0m, in \u001b[0;36mEndpoint.predict\u001b[0;34m(self, instances, parameters, timeout, use_raw_predict)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[39mreturn\u001b[39;00m Prediction(\n\u001b[1;32m   1552\u001b[0m         predictions\u001b[39m=\u001b[39mjson_response[\u001b[39m\"\u001b[39m\u001b[39mpredictions\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m   1553\u001b[0m         deployed_model_id\u001b[39m=\u001b[39mraw_predict_response\u001b[39m.\u001b[39mheaders[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1561\u001b[0m         ),\n\u001b[1;32m   1562\u001b[0m     )\n\u001b[1;32m   1563\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1564\u001b[0m     prediction_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prediction_client\u001b[39m.\u001b[39mpredict(\n\u001b[1;32m   1565\u001b[0m         endpoint\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gca_resource\u001b[39m.\u001b[39mname,\n\u001b[1;32m   1566\u001b[0m         instances\u001b[39m=\u001b[39minstances,\n\u001b[1;32m   1567\u001b[0m         parameters\u001b[39m=\u001b[39mparameters,\n\u001b[1;32m   1568\u001b[0m         timeout\u001b[39m=\u001b[39mtimeout,\n\u001b[1;32m   1569\u001b[0m     )\n\u001b[1;32m   1571\u001b[0m     \u001b[39mreturn\u001b[39;00m Prediction(\n\u001b[1;32m   1572\u001b[0m         predictions\u001b[39m=\u001b[39m[\n\u001b[1;32m   1573\u001b[0m             json_format\u001b[39m.\u001b[39mMessageToDict(item)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1578\u001b[0m         model_resource_name\u001b[39m=\u001b[39mprediction_response\u001b[39m.\u001b[39mmodel,\n\u001b[1;32m   1579\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/google/cloud/aiplatform_v1/services/prediction_service/client.py:589\u001b[0m, in \u001b[0;36mPredictionServiceClient.predict\u001b[0;34m(self, request, endpoint, instances, parameters, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    587\u001b[0m     request\u001b[39m.\u001b[39mendpoint \u001b[39m=\u001b[39m endpoint\n\u001b[1;32m    588\u001b[0m \u001b[39mif\u001b[39;00m instances \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 589\u001b[0m     request\u001b[39m.\u001b[39minstances\u001b[39m.\u001b[39mextend(instances)\n\u001b[1;32m    590\u001b[0m \u001b[39mif\u001b[39;00m parameters \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    591\u001b[0m     request\u001b[39m.\u001b[39mparameters \u001b[39m=\u001b[39m parameters\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/proto/message.py:752\u001b[0m, in \u001b[0;36mMessage.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    750\u001b[0m pb_value \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pb, key)\n\u001b[1;32m    751\u001b[0m marshal \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_meta\u001b[39m.\u001b[39mmarshal\n\u001b[0;32m--> 752\u001b[0m \u001b[39mreturn\u001b[39;00m marshal\u001b[39m.\u001b[39mto_python(pb_type, pb_value, absent\u001b[39m=\u001b[39mkey \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/proto/marshal/marshal.py:193\u001b[0m, in \u001b[0;36mBaseMarshal.to_python\u001b[0;34m(self, proto_type, value, absent)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[39mif\u001b[39;00m value_type \u001b[39min\u001b[39;00m compat\u001b[39m.\u001b[39mmap_composite_types:\n\u001b[1;32m    192\u001b[0m     \u001b[39mreturn\u001b[39;00m MapComposite(value, marshal\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n\u001b[0;32m--> 193\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_rule(proto_type\u001b[39m=\u001b[39mproto_type)\u001b[39m.\u001b[39mto_python(value, absent\u001b[39m=\u001b[39mabsent)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/google/cloud/aiplatform/utils/enhanced_library/_decorators.py:24\u001b[0m, in \u001b[0;36mConversionValueRule.to_python\u001b[0;34m(self, value, absent)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mto_python\u001b[39m(\u001b[39mself\u001b[39m, value, \u001b[39m*\u001b[39m, absent: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m---> 24\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mto_python(value, absent\u001b[39m=\u001b[39mabsent)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/proto/marshal/rules/struct.py:39\u001b[0m, in \u001b[0;36mValueRule.to_python\u001b[0;34m(self, value, absent)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mto_python\u001b[39m(\u001b[39mself\u001b[39m, value, \u001b[39m*\u001b[39m, absent: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     30\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Coerce the given value to the appropriate Python type.\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \n\u001b[1;32m     32\u001b[0m \u001b[39m    Note that both NullValue and absent fields return None.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[39m    which is True for NullValue and False for an absent value.\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m     kind \u001b[39m=\u001b[39m value\u001b[39m.\u001b[39mWhichOneof(\u001b[39m\"\u001b[39m\u001b[39mkind\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     40\u001b[0m     \u001b[39mif\u001b[39;00m kind \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnull_value\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m absent:\n\u001b[1;32m     41\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'google.protobuf.pyext._message.RepeatedCompositeCo' object has no attribute 'WhichOneof'"
     ]
    }
   ],
   "source": [
    "question = \"What NFL team won the Super Bowl in the year Justin Beiber was born?\"\n",
    "\n",
    "llm_chain.run(question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
